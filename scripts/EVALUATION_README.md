# DealerScope AI Evaluation Suite

A comprehensive testing framework that evaluates the completeness and functionality of the DealerScope application against multiple AI models and performance criteria.

## ðŸš€ Quick Start

```bash
# Make the script executable
chmod +x scripts/run-evaluation.sh

# Run the evaluation suite
./scripts/run-evaluation.sh

# Or run with verbose output
./scripts/run-evaluation.sh --verbose
```

## ðŸ“‹ What It Tests

### Frontend Functionality
- âœ… Page loading and rendering
- âœ… React component functionality
- âœ… PWA features (manifest, service worker)
- âœ… User interface responsiveness
- âœ… Navigation and routing

### Backend API
- âœ… Health endpoint availability
- âœ… API response times
- âœ… Data processing endpoints
- âœ… Error handling
- âœ… Database connectivity

### Security Compliance
- âœ… Content Security Policy (CSP) headers
- âœ… Input validation and sanitization
- âœ… XSS protection
- âœ… CSRF protection
- âœ… Authentication mechanisms

### Performance Metrics
- âœ… Page load times
- âœ… API response times
- âœ… Bundle size optimization
- âœ… Memory usage
- âœ… Network efficiency

### Data Processing
- âœ… CSV upload and parsing
- âœ… Data validation
- âœ… Arbitrage calculations
- âœ… Market analysis
- âœ… Report generation

### Error Handling
- âœ… Error boundaries
- âœ… Graceful degradation
- âœ… Recovery mechanisms
- âœ… User feedback
- âœ… Logging and monitoring

## ðŸ¤– AI Model Integration

The suite can evaluate your application against multiple AI models:

### OpenAI Models
- GPT-5 (flagship model with superior reasoning)
- GPT-5 Mini (fast and efficient)
- GPT-4.1 (reliable results)
- O3 (powerful reasoning model)

### Anthropic Models
- Claude Opus 4 (most capable)
- Claude Sonnet 4 (high performance)
- Claude Haiku 3.5 (fastest responses)

### Perplexity Models
- Llama 3.1 Sonar (with online search)

## ðŸ“Š Scoring System

The evaluation uses a weighted scoring system:

- **Functionality (30%)** - Core features and user interactions
- **Reliability (25%)** - Error handling and stability
- **Performance (20%)** - Speed and efficiency
- **Security (15%)** - Protection and compliance
- **Completeness (10%)** - Feature coverage and documentation

### Completeness Levels

| Score | Level | Description |
|-------|-------|-------------|
| 90%+ | Production Ready | Fully complete and production-ready |
| 75-89% | Near Complete | Minor issues, mostly ready |
| 60-74% | Functional | Core functionality works |
| 40-59% | Basic Implementation | Basic features present |
| <40% | Incomplete | Significant work needed |

## ðŸ“ Report Output

The evaluation generates comprehensive reports in `./evaluation-reports/`:

```
evaluation-reports/
â”œâ”€â”€ YYYY-MM-DD/
â”‚   â”œâ”€â”€ frontend-functionality-report.json
â”‚   â”œâ”€â”€ backend-api-report.json
â”‚   â”œâ”€â”€ security-compliance-report.json
â”‚   â”œâ”€â”€ performance-metrics-report.json
â”‚   â”œâ”€â”€ data-processing-report.json
â”‚   â”œâ”€â”€ error-handling-report.json
â”‚   â”œâ”€â”€ final-report.json
â”‚   â””â”€â”€ final-report.html
```

### HTML Report Features
- ðŸ“Š Visual scoring dashboard
- ðŸ§ª Detailed test results
- ðŸ”§ Actionable recommendations
- ðŸ“ˆ Performance benchmarks
- ðŸ”’ Security audit results

## âš™ï¸ Configuration

Edit `scripts/evaluation-config.json` to customize:

- AI models to test against
- Test criteria and weights
- Performance benchmarks
- Security requirements
- API endpoints to validate

## ðŸ”§ Prerequisites

- Node.js (v16 or higher)
- npm or yarn
- DealerScope application running
- Network access for AI model APIs (optional)

## ðŸš¦ Running Individual Test Suites

```bash
# Test only frontend functionality
node scripts/ai-evaluation-suite.js --suite frontend-functionality

# Test only security compliance
node scripts/ai-evaluation-suite.js --suite security-compliance

# Test with specific AI model
node scripts/ai-evaluation-suite.js --model gpt-5-2025-08-07
```

## ðŸ“ Example Output

```
ðŸš€ Initializing DealerScope AI Evaluation Suite...
ðŸ”§ Starting DealerScope services...
â³ Waiting for services to be ready...
âœ… Services ready!

ðŸ§ª Running comprehensive evaluation suite...

ðŸ“‹ Running frontend-functionality tests...
âœ… Page Load: PASS (10/10)
âœ… Component Rendering: PASS (15/15)
âœ… PWA Features: PASS (10/10)

ðŸ“‹ Running backend-api tests...
âŒ Health Endpoint: FAIL (0/15) - Backend not available
âœ… Dashboard Endpoint: PASS (10/10)

ðŸ“Š Generating final evaluation report...
ðŸ“‹ Final report generated: ./evaluation-reports/2025-08-20/final-report.html

ðŸŽ‰ Evaluation Complete!
Overall Score: 85%
Completeness: Near Complete
```

## ðŸ” Troubleshooting

### Common Issues

**"Services failed to start"**
- Ensure all dependencies are installed: `npm install`
- Check if ports 3000/8000 are available
- Verify backend script exists and is executable

**"Backend not available"**
- Backend script may not be present (frontend-only evaluation will continue)
- Check if Python dependencies are installed for backend
- Verify database connectivity

**"Permission denied"**
- Make script executable: `chmod +x scripts/run-evaluation.sh`
- Check file permissions on evaluation suite

### Debug Mode

```bash
# Enable verbose logging
./scripts/run-evaluation.sh --verbose

# Check individual components
curl http://localhost:3000  # Frontend
curl http://localhost:8000/health  # Backend
```

## ðŸ¤ Contributing

To add new test suites or AI model integrations:

1. Edit `scripts/ai-evaluation-suite.js`
2. Add new test methods to the `AIEvaluationSuite` class
3. Update `evaluation-config.json` with new criteria
4. Run the suite to verify changes

## ðŸ“„ License

Part of the DealerScope project. See main project license for details.