name: Production-Grade Validation Pipeline

on:
  pull_request:
    types: [opened, synchronize, reopened, ready_for_review]
    paths-ignore:
      - '**.md'
      - 'docs/**'
      - 'LICENSE'
      - '.gitignore'
  push:
    branches: [main, develop]
    paths-ignore:
      - '**.md'
      - 'docs/**'
      - 'LICENSE'
      - '.gitignore'
  workflow_dispatch:
    inputs:
      debug_enabled:
        type: boolean
        description: 'Enable debug mode'
        required: false
        default: false
      skip_security_scan:
        type: boolean
        description: 'Skip security scanning (emergency only)'
        required: false
        default: false

# Security: Minimal permissions by default
permissions:
  contents: read
  pull-requests: write
  checks: write
  security-events: write
  
concurrency:
  group: validation-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ github.event_name == 'pull_request' }}

env:
  NODE_ENV: production
  PYTHONUNBUFFERED: 1
  FORCE_COLOR: 1
  TERM: xterm-256color
  CI: true
  # Security: Disable telemetry
  NEXT_TELEMETRY_DISABLED: 1
  DO_NOT_TRACK: 1
  # Performance: Parallel execution
  PYTEST_XDIST_WORKER_COUNT: auto
  NODE_OPTIONS: --max-old-space-size=4096
  # Enhanced validation settings
  VALIDATE_CONTRIBUTING: true
  ENFORCE_SECURITY_GATES: true

jobs:
  # Job: Enhanced security scanning before any code execution
  security-scan:
    runs-on: ubuntu-latest
    if: ${{ !github.event.inputs.skip_security_scan }}
    permissions:
      security-events: write
      contents: read
    outputs:
      security-status: ${{ steps.security-summary.outputs.status }}
      vulnerabilities-found: ${{ steps.security-summary.outputs.vulnerabilities }}
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          fetch-depth: 0
          
      - name: Setup Node.js for security tools
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          
      - name: Install security scanning tools
        run: |
          npm install -g npm-audit-resolver semver
          pip install --upgrade pip safety bandit
          
      - name: Run comprehensive vulnerability scanning
        id: vuln-scan
        run: |
          mkdir -p security-reports
          
          # Enhanced npm audit with detailed reporting
          if [ -f "package-lock.json" ]; then
            echo "üîç Scanning npm dependencies..."
            npm audit --audit-level=moderate --json > security-reports/npm-audit.json || true
            npm audit --audit-level=moderate || echo "::warning::npm vulnerabilities detected"
          fi
          
          # Python dependency scanning with Safety
          if [ -f "requirements.txt" ]; then
            echo "üîç Scanning Python dependencies..."
            safety check --json --output security-reports/safety-report.json || echo "::warning::Python vulnerabilities detected"
            safety check || echo "::warning::Python vulnerabilities detected"
          fi
          
          # License compliance check
          if [ -f "package.json" ]; then
            echo "üîç Checking license compliance..."
            npx license-checker --json > security-reports/license-report.json || true
          fi
          
      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH,MEDIUM'
          
      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'
          
      - name: Run Bandit security linter (Python)
        if: hashFiles('**/*.py') != ''
        run: |
          echo "üîç Running Bandit security analysis..."
          bandit -r . -f json -o security-reports/bandit-report.json || true
          bandit -r . -ll || echo "::warning::Python security issues detected"
          
      - name: CodeQL Analysis
        uses: github/codeql-action/init@v3
        with:
          languages: 'javascript,python'
          queries: security-and-quality
          
      - name: Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v3
        with:
          category: '/language:javascript,python'
          
      - name: Security summary and gates
        id: security-summary
        run: |
          echo "üìä Security Scan Summary" > security-reports/summary.md
          echo "=========================" >> security-reports/summary.md
          
          # Count vulnerabilities
          npm_vulns=0
          python_vulns=0
          
          if [ -f "security-reports/npm-audit.json" ]; then
            npm_vulns=$(jq '.metadata.vulnerabilities.total // 0' security-reports/npm-audit.json)
          fi
          
          if [ -f "security-reports/safety-report.json" ]; then
            python_vulns=$(jq '. | length' security-reports/safety-report.json 2>/dev/null || echo 0)
          fi
          
          total_vulns=$((npm_vulns + python_vulns))
          
          echo "- NPM vulnerabilities: $npm_vulns" >> security-reports/summary.md
          echo "- Python vulnerabilities: $python_vulns" >> security-reports/summary.md
          echo "- Total vulnerabilities: $total_vulns" >> security-reports/summary.md
          
          # Set security gate
          if [ "$total_vulns" -gt 5 ]; then
            echo "status=failed" >> $GITHUB_OUTPUT
            echo "::error::Security gate failed: Too many vulnerabilities ($total_vulns > 5)"
          else
            echo "status=passed" >> $GITHUB_OUTPUT
            echo "‚úÖ Security gate passed"
          fi
          
          echo "vulnerabilities=$total_vulns" >> $GITHUB_OUTPUT
          
      - name: Upload security reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-reports
          path: security-reports/
          retention-days: 30

  validate-frontend:
    needs: [security-scan]
    runs-on: ubuntu-latest
    timeout-minutes: 25
    strategy:
      fail-fast: false
      matrix:
        node-version: ['18.x', '20.x']
    outputs:
      build-status: ${{ steps.build-check.outputs.status }}
      test-coverage: ${{ steps.test-coverage.outputs.coverage }}
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          
      - name: Setup Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'
          
      - name: Enhanced dependency caching
        uses: actions/cache@v4
        with:
          path: |
            ~/.npm
            node_modules
            .next/cache
            .vite/deps
          key: ${{ runner.os }}-node-${{ matrix.node-version }}-${{ hashFiles('**/package-lock.json') }}-v2
          restore-keys: |
            ${{ runner.os }}-node-${{ matrix.node-version }}-v2
            ${{ runner.os }}-node-v2
            
      - name: Install dependencies with enhanced security
        run: |
          echo "üîß Installing dependencies with enhanced security checks..."
          npm ci --prefer-offline --no-audit --ignore-scripts
          
          # Verify package integrity
          npm ls --depth=0 || true
          
          # Check for package vulnerabilities again after install
          npm audit --audit-level=high || echo "::warning::High severity vulnerabilities detected"
          
      - name: Enhanced code quality validation
        run: |
          echo "üîç Running enhanced code quality checks..."
          
          # TypeScript compilation check
          if npx tsc --noEmit --skipLibCheck; then
            echo "‚úÖ TypeScript compilation passed"
          else
            echo "::error::TypeScript compilation failed"
            exit 1
          fi
          
          # ESLint with enhanced rules
          if npm run lint; then
            echo "‚úÖ ESLint validation passed"
          else
            echo "::warning::ESLint found issues - reviewing for blockers"
            # Allow warnings but fail on errors
            npm run lint -- --max-warnings 50 || exit 1
          fi
          
      - name: Enhanced testing with coverage
        id: test-coverage
        run: |
          echo "üß™ Running enhanced test suite..."
          
          # Create test reports directory
          mkdir -p test-reports/coverage
          
          # Run tests with coverage (if vitest is configured)
          if npm run test:coverage 2>/dev/null; then
            echo "‚úÖ Tests with coverage completed"
          elif npx vitest run --coverage 2>/dev/null; then
            echo "‚úÖ Vitest coverage completed"
          else
            echo "‚ö†Ô∏è No test coverage command found, running basic tests"
            npm test 2>/dev/null || echo "No tests configured"
          fi
          
          # Extract coverage percentage if available
          if [ -f "coverage/lcov-report/index.html" ]; then
            coverage=$(grep -o '[0-9]\+\.[0-9]\+%' coverage/lcov-report/index.html | head -1 | sed 's/%//' || echo "0")
            echo "coverage=$coverage" >> $GITHUB_OUTPUT
            echo "üìä Test coverage: $coverage%"
          else
            echo "coverage=0" >> $GITHUB_OUTPUT
          fi
          
      - name: Enhanced build validation
        id: build-check
        run: |
          echo "üèóÔ∏è Running enhanced build validation..."
          
          # Build the application
          if npm run build; then
            echo "‚úÖ Build successful"
            echo "status=success" >> $GITHUB_OUTPUT
          else
            echo "::error::Build failed"
            echo "status=failed" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          # Verify build output structure
          if [ -d "dist" ] || [ -d "build" ] || [ -d ".next" ]; then
            echo "‚úÖ Build artifacts verified"
          else
            echo "::error::Build artifacts not found"
            exit 1
          fi
          
          # Check bundle size if configured
          if command -v npx >/dev/null && [ -f "package.json" ]; then
            npx size-limit --json > size-report.json 2>/dev/null || echo "üìä Bundle size check skipped"
          fi
          
      - name: Bundle analysis and performance
        run: |
          echo "üìä Analyzing bundle performance..."
          
          # Generate bundle analysis if tools are available
          if [ -f "dist/index.html" ]; then
            # Basic bundle size check
            dist_size=$(du -sh dist/ 2>/dev/null | cut -f1 || echo "unknown")
            echo "üì¶ Bundle size: $dist_size"
            
            # Check for large files
            find dist/ -type f -size +1M 2>/dev/null | while read -r file; do
              size=$(du -h "$file" | cut -f1)
              echo "‚ö†Ô∏è Large asset: $file ($size)"
            done
          fi
          
      - name: Frontend security validation
        run: |
          echo "üîí Running frontend security validation..."
          
          # Check for common security issues in built files
          if [ -d "dist" ]; then
            # Check for exposed sensitive files
            find dist/ -name "*.env*" -o -name "*.key" -o -name "*.pem" 2>/dev/null | while read -r file; do
              echo "::error::Sensitive file found in build: $file"
              exit 1
            done
            
            # Check for potential XSS vulnerabilities in HTML
            if find dist/ -name "*.html" -exec grep -l "eval\|innerHTML\|document.write" {} \; | head -1; then
              echo "::warning::Potential XSS vulnerabilities detected in HTML"
            fi
          fi
          
      - name: Generate frontend validation report
        if: always()
        run: |
          mkdir -p validation-reports/frontend
          
          # Consolidate reports
          cat > validation-reports/frontend/summary-${{ matrix.node-version }}.json << EOF
          {
            "node_version": "${{ matrix.node-version }}",
            "build_status": "${{ steps.build-check.outputs.status }}",
            "test_coverage": "${{ steps.test-coverage.outputs.coverage }}",
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "lint_status": "completed",
            "security_check": "completed"
          }
          EOF
          
          # Copy coverage reports if available
          cp -r coverage/* validation-reports/frontend/ 2>/dev/null || true
          cp size-report.json validation-reports/frontend/ 2>/dev/null || true
          
      - name: Upload frontend validation artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: frontend-validation-${{ matrix.node-version }}
          path: validation-reports/
          retention-days: 7
          
      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.npm
            node_modules
            .next/cache
          key: ${{ runner.os }}-node-${{ matrix.node-version }}-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-node-${{ matrix.node-version }}-
            
      - name: Install dependencies with integrity check
        run: |
          npm ci --prefer-offline --no-audit --silent
          npm ls --depth=0 || true
          
      - name: Parallel validation tasks
        run: |
          # Run multiple validation tasks in parallel
          npm run lint &
          lint_pid=$!
          
          npm run type-check &
          type_pid=$!
          
          npm run test:unit -- --coverage --maxWorkers=50% &
          test_pid=$!
          
          # Wait for all background jobs
          wait $lint_pid || echo "::error::Linting failed"
          wait $type_pid || echo "::error::Type checking failed"
          wait $test_pid || echo "::error::Unit tests failed"
          
      - name: Build validation
        run: |
          npm run build
          # Verify build output
          test -d dist || { echo "::error::Build output missing"; exit 1; }
          
      - name: Bundle size check
        run: |
          # Check bundle sizes against thresholds
          npx size-limit || echo "::warning::Bundle size exceeds limits"
          
      - name: Generate comprehensive reports
        if: always()
        run: |
          mkdir -p validation-reports/{coverage,lint,security,performance}
          
          # Coverage report
          if [ -f coverage/lcov.info ]; then
            npx lcov-summary coverage/lcov.info > validation-reports/coverage/summary.txt
            cp -r coverage/* validation-reports/coverage/ 2>/dev/null || true
          fi
          
          # Lighthouse CI for performance
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            npx lhci autorun --collect.numberOfRuns=3 || true
          fi
          
          # Generate consolidated report
          node scripts/generate-frontend-report.js || echo '{"status":"partial"}' > validation-reports/summary.json
          
      - name: Upload frontend artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: frontend-reports-${{ matrix.node-version }}
          path: validation-reports/
          retention-days: 7
          compression-level: 9

  validate-backend:
    needs: [security-scan]
    runs-on: ubuntu-latest
    timeout-minutes: 25
    strategy:
      matrix:
        python-version: ['3.11', '3.12']
        test-group: ['unit', 'integration', 'performance']
    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_USER: testuser
          POSTGRES_PASSWORD: testpass
          POSTGRES_DB: testdb
        ports:
          - 5432:5432
        options: >-
          --health-cmd="pg_isready -U testuser"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=5
          
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd="redis-cli ping"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=5
          
    env:
      DATABASE_URL: postgresql://testuser:testpass@localhost:5432/testdb
      REDIS_URL: redis://localhost:6379
      PYTHONPATH: ${{ github.workspace }}
      
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          
      - name: Setup Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            requirements-dev.txt
            
      - name: Cache Python packages
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            .pytest_cache
            .mypy_cache
            .ruff_cache
          key: ${{ runner.os }}-python-${{ matrix.python-version }}-${{ hashFiles('requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-python-${{ matrix.python-version }}-
            
      - name: Install dependencies with verification
        run: |
          python -m pip install --upgrade pip wheel setuptools
          pip install -r requirements.txt -r requirements-dev.txt
          pip check
          pip list --outdated || true
          
      - name: Static analysis
        if: matrix.test-group == 'unit'
        run: |
          # Type checking
          mypy src --ignore-missing-imports --strict &
          mypy_pid=$!
          
          # Security scanning
          bandit -r src -ll &
          bandit_pid=$!
          
          # Code quality
          ruff check src &
          ruff_pid=$!
          
          # Complexity analysis
          radon cc src -s -nb &
          radon_pid=$!
          
          wait $mypy_pid || echo "::warning::Type checking issues found"
          wait $bandit_pid || echo "::warning::Security issues found"
          wait $ruff_pid || echo "::warning::Code quality issues found"
          wait $radon_pid || true
          
      - name: Database migrations
        if: matrix.test-group != 'unit'
        run: |
          alembic upgrade head
          alembic check
          
      - name: Run test suite - ${{ matrix.test-group }}
        run: |
          case "${{ matrix.test-group }}" in
            unit)
              pytest tests/unit \
                --cov=src \
                --cov-report=xml:coverage.xml \
                --cov-report=html:htmlcov \
                --cov-report=term-missing \
                --cov-fail-under=80 \
                --maxfail=5 \
                --tb=short \
                -n auto \
                --dist loadgroup \
                --junitxml=test-results.xml
              ;;
            integration)
              pytest tests/integration \
                --maxfail=3 \
                --tb=short \
                -n 2 \
                --junitxml=test-results.xml
              ;;
            performance)
              pytest tests/performance \
                --benchmark-only \
                --benchmark-json=benchmark.json \
                --benchmark-autosave \
                --benchmark-compare-fail=min:10% \
                --junitxml=test-results.xml
              ;;
          esac
          
      - name: Generate backend reports
        if: always()
        run: |
          mkdir -p validation-reports/{coverage,tests,security,performance}
          
          # Copy test results
          cp test-results.xml validation-reports/tests/ 2>/dev/null || true
          cp coverage.xml validation-reports/coverage/ 2>/dev/null || true
          cp -r htmlcov validation-reports/coverage/ 2>/dev/null || true
          cp benchmark.json validation-reports/performance/ 2>/dev/null || true
          
          # Generate summary
          python scripts/generate_backend_report.py || echo '{"status":"partial"}' > validation-reports/summary.json
          
      - name: Upload backend artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: backend-reports-${{ matrix.python-version }}-${{ matrix.test-group }}
          path: validation-reports/
          retention-days: 7
          compression-level: 9

  performance-analysis:
    needs: [validate-frontend, validate-backend]
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: all-reports
          
      - name: Analyze performance regression
        run: |
          # Compare with base branch metrics
          python scripts/performance_regression.py \
            --current all-reports \
            --threshold 5 \
            --output performance-analysis.md
            
      - name: Comment PR with analysis
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const analysis = fs.readFileSync('performance-analysis.md', 'utf8');
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: analysis
            });

  enhanced-gatekeeper:
    needs: [validate-frontend, validate-backend, security-scan]
    runs-on: ubuntu-latest
    if: always()
    permissions:
      checks: write
      pull-requests: write
      contents: read
    outputs:
      quality-gate-status: ${{ steps.quality-gates.outputs.status }}
      security-gate-status: ${{ steps.security-gates.outputs.status }}
      deployment-ready: ${{ steps.deployment-readiness.outputs.ready }}
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false
          
      - name: Setup Python for report processing
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          
      - name: Install report processing tools
        run: |
          pip install jq-python pyyaml requests
          
      - name: Download all validation artifacts
        uses: actions/download-artifact@v4
        if: always()
        with:
          path: all-reports
          
      - name: Enhanced quality gates validation
        id: quality-gates
        run: |
          echo "üö™ Running Enhanced Quality Gates..."
          mkdir -p final-reports
          
          # Enhanced quality gate configuration
          MIN_COVERAGE=70
          MAX_CRITICAL_VULNERABILITIES=0
          MAX_HIGH_VULNERABILITIES=2
          MAX_BUILD_FAILURES=0
          MIN_NODE_VERSIONS_PASSING=1
          MAX_ESLINT_ERRORS=0
          
          # Initialize counters
          failed=false
          warnings=false
          total_coverage=0
          coverage_count=0
          critical_vulns=0
          high_vulns=0
          build_failures=0
          node_passing=0
          eslint_errors=0
          
          # Process frontend reports
          echo "üìä Processing frontend validation reports..."
          for report in all-reports/frontend-validation-*/summary-*.json; do
            if [ -f "$report" ]; then
              echo "Processing: $report"
              
              # Extract coverage if available  
              coverage=$(jq -r '.test_coverage // 0' "$report" 2>/dev/null || echo "0")
              if [ "$coverage" != "0" ] && [ "$coverage" != "null" ]; then
                total_coverage=$(echo "$total_coverage + $coverage" | bc -l 2>/dev/null || echo "$total_coverage")
                coverage_count=$((coverage_count + 1))
              fi
              
              # Check build status
              build_status=$(jq -r '.build_status // "unknown"' "$report" 2>/dev/null || echo "unknown")
              if [ "$build_status" = "success" ]; then
                node_passing=$((node_passing + 1))
              elif [ "$build_status" = "failed" ]; then
                build_failures=$((build_failures + 1))
              fi
            fi
          done
          
          # Process security reports
          echo "üîí Processing security validation reports..."
          if [ -f "all-reports/security-reports/summary.md" ]; then
            # Extract vulnerability counts from security summary
            if [ -f "all-reports/security-reports/npm-audit.json" ]; then
              npm_critical=$(jq -r '.metadata.vulnerabilities.critical // 0' all-reports/security-reports/npm-audit.json 2>/dev/null || echo "0")
              npm_high=$(jq -r '.metadata.vulnerabilities.high // 0' all-reports/security-reports/npm-audit.json 2>/dev/null || echo "0")
              critical_vulns=$((critical_vulns + npm_critical))
              high_vulns=$((high_vulns + npm_high))
            fi
          fi
          
          # Calculate average coverage
          if [ "$coverage_count" -gt 0 ]; then
            avg_coverage=$(echo "scale=2; $total_coverage / $coverage_count" | bc -l 2>/dev/null || echo "0")
          else
            avg_coverage=0
          fi
          
          # Generate quality gate report
          cat > final-reports/quality-gates-report.md << EOF
          # üö™ Enhanced Quality Gates Report
          
          ## Coverage Analysis
          - **Average Test Coverage**: ${avg_coverage}% (Minimum: ${MIN_COVERAGE}%)
          - **Coverage Reports Analyzed**: ${coverage_count}
          
          ## Security Analysis  
          - **Critical Vulnerabilities**: ${critical_vulns} (Maximum: ${MAX_CRITICAL_VULNERABILITIES})
          - **High Vulnerabilities**: ${high_vulns} (Maximum: ${MAX_HIGH_VULNERABILITIES})
          
          ## Build Analysis
          - **Node.js Versions Passing**: ${node_passing} (Minimum: ${MIN_NODE_VERSIONS_PASSING})
          - **Build Failures**: ${build_failures} (Maximum: ${MAX_BUILD_FAILURES})
          
          ## Quality Gate Results
          EOF
          
          # Enforce quality gates
          echo "üîç Enforcing quality gates..."
          
          # Coverage gate
          if (( $(echo "$avg_coverage < $MIN_COVERAGE" | bc -l 2>/dev/null || echo "1") )); then
            echo "‚ùå **Coverage Gate: FAILED** - ${avg_coverage}% < ${MIN_COVERAGE}%" >> final-reports/quality-gates-report.md
            echo "::error::Coverage gate failed: ${avg_coverage}% is below minimum ${MIN_COVERAGE}%"
            failed=true
          else
            echo "‚úÖ **Coverage Gate: PASSED** - ${avg_coverage}% >= ${MIN_COVERAGE}%" >> final-reports/quality-gates-report.md
          fi
          
          # Security gates
          if [ "$critical_vulns" -gt "$MAX_CRITICAL_VULNERABILITIES" ]; then
            echo "‚ùå **Critical Security Gate: FAILED** - ${critical_vulns} critical vulnerabilities found" >> final-reports/quality-gates-report.md
            echo "::error::Critical security gate failed: ${critical_vulns} critical vulnerabilities (max: ${MAX_CRITICAL_VULNERABILITIES})"
            failed=true
          else
            echo "‚úÖ **Critical Security Gate: PASSED** - ${critical_vulns} critical vulnerabilities" >> final-reports/quality-gates-report.md
          fi
          
          if [ "$high_vulns" -gt "$MAX_HIGH_VULNERABILITIES" ]; then
            echo "‚ö†Ô∏è **High Security Gate: WARNING** - ${high_vulns} high vulnerabilities found" >> final-reports/quality-gates-report.md
            echo "::warning::High security gate warning: ${high_vulns} high vulnerabilities (max: ${MAX_HIGH_VULNERABILITIES})"
            warnings=true
          else
            echo "‚úÖ **High Security Gate: PASSED** - ${high_vulns} high vulnerabilities" >> final-reports/quality-gates-report.md
          fi
          
          # Build gates
          if [ "$build_failures" -gt "$MAX_BUILD_FAILURES" ]; then
            echo "‚ùå **Build Gate: FAILED** - ${build_failures} build failures" >> final-reports/quality-gates-report.md
            echo "::error::Build gate failed: ${build_failures} build failures"
            failed=true
          else
            echo "‚úÖ **Build Gate: PASSED** - ${build_failures} build failures" >> final-reports/quality-gates-report.md
          fi
          
          if [ "$node_passing" -lt "$MIN_NODE_VERSIONS_PASSING" ]; then
            echo "‚ùå **Node.js Compatibility Gate: FAILED** - Only ${node_passing} versions passing" >> final-reports/quality-gates-report.md
            echo "::error::Node.js compatibility gate failed: ${node_passing} versions passing (min: ${MIN_NODE_VERSIONS_PASSING})"
            failed=true
          else
            echo "‚úÖ **Node.js Compatibility Gate: PASSED** - ${node_passing} versions passing" >> final-reports/quality-gates-report.md
          fi
          
          # Overall status
          if [ "$failed" = true ]; then
            echo "status=failed" >> $GITHUB_OUTPUT
            echo "" >> final-reports/quality-gates-report.md
            echo "## üö® Overall Status: FAILED" >> final-reports/quality-gates-report.md
            echo "One or more quality gates have failed. Please address the issues above before merging." >> final-reports/quality-gates-report.md
            exit 1
          elif [ "$warnings" = true ]; then
            echo "status=warning" >> $GITHUB_OUTPUT
            echo "" >> final-reports/quality-gates-report.md
            echo "## ‚ö†Ô∏è Overall Status: PASSED WITH WARNINGS" >> final-reports/quality-gates-report.md
            echo "All critical gates passed, but some warnings were found." >> final-reports/quality-gates-report.md
          else
            echo "status=passed" >> $GITHUB_OUTPUT
            echo "" >> final-reports/quality-gates-report.md
            echo "## ‚úÖ Overall Status: PASSED" >> final-reports/quality-gates-report.md
            echo "All quality gates have passed successfully!" >> final-reports/quality-gates-report.md
          fi
          
      - name: Contributing guidelines validation
        id: contributing-validation
        run: |
          echo "üìã Validating contributing guidelines compliance..."
          
          # Check if CONTRIBUTING.md exists and is up to date
          if [ -f "CONTRIBUTING.md" ]; then
            echo "‚úÖ CONTRIBUTING.md exists"
            
            # Check if dependabot.yml exists
            if [ -f ".github/dependabot.yml" ]; then
              echo "‚úÖ Dependabot configuration exists"
            else
              echo "::warning::dependabot.yml not found"
            fi
            
            # Validate PR has proper description (for PR events)
            if [ "${{ github.event_name }}" = "pull_request" ]; then
              pr_body="${{ github.event.pull_request.body }}"
              if [ -n "$pr_body" ] && [ ${#pr_body} -gt 50 ]; then
                echo "‚úÖ PR has adequate description"
              else
                echo "::warning::PR description is too short or missing"
              fi
            fi
          else
            echo "::error::CONTRIBUTING.md not found"
            exit 1
          fi
          
      - name: Deployment readiness assessment
        id: deployment-readiness
        run: |
          echo "üöÄ Assessing deployment readiness..."
          
          quality_status="${{ steps.quality-gates.outputs.status }}"
          security_status="${{ needs.security-scan.outputs.security-status }}"
          
          if [ "$quality_status" = "passed" ] && [ "$security_status" = "passed" ]; then
            echo "ready=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Deployment ready - all gates passed"
          elif [ "$quality_status" = "warning" ] && [ "$security_status" = "passed" ]; then
            echo "ready=conditional" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è Deployment ready with warnings"
          else
            echo "ready=false" >> $GITHUB_OUTPUT
            echo "‚ùå Deployment not ready - quality or security gates failed"
          fi
          
      - name: Generate enhanced deployment manifest
        if: success() && github.ref == 'refs/heads/main'
        run: |
          echo "üì¶ Generating enhanced deployment manifest..."
          
          # Create comprehensive deployment manifest
          cat > deployment-manifest.json << EOF
          {
            "version": "${{ github.sha }}",
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "validated": true,
            "quality_gates": {
              "status": "${{ steps.quality-gates.outputs.status }}",
              "security_status": "${{ needs.security-scan.outputs.security-status }}",
              "deployment_ready": "${{ steps.deployment-readiness.outputs.ready }}"
            },
            "validation_results": {
              "frontend_builds": ${node_passing},
              "security_vulnerabilities": ${critical_vulns},
              "test_coverage": ${avg_coverage}
            },
            "artifacts": {
              "frontend_reports": "frontend-validation-*",
              "security_reports": "security-reports",
              "quality_gates": "final-reports"
            },
            "signatures": {
              "sha256": "$(find all-reports -type f -exec sha256sum {} \; | sha256sum | cut -d' ' -f1 2>/dev/null || echo 'unavailable')"
            },
            "environment": {
              "node_env": "production",
              "validated_at": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
            }
          }
          EOF
          
      - name: Upload enhanced reports and manifests
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: enhanced-validation-reports
          path: |
            final-reports/
            deployment-manifest.json
          retention-days: 90
          
      - name: Comment PR with validation results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let comment = '## üö™ Enhanced Quality Gates Report\n\n';
            
            try {
              if (fs.existsSync('final-reports/quality-gates-report.md')) {
                const report = fs.readFileSync('final-reports/quality-gates-report.md', 'utf8');
                comment += report;
              } else {
                comment += '‚ö†Ô∏è Quality gates report not available\n';
              }
              
              comment += '\n\n---\n';
              comment += `**Validation completed at:** ${new Date().toISOString()}\n`;
              comment += `**Commit:** ${{ github.sha }}\n`;
              comment += `**Workflow:** [View Details](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})\n`;
              
            } catch (error) {
              comment += `‚ùå Error generating report: ${error.message}\n`;
            }
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
            
      - name: Create enhanced check run summary
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const status = '${{ steps.quality-gates.outputs.status }}';
            const securityStatus = '${{ needs.security-scan.outputs.security-status }}';
            const deploymentReady = '${{ steps.deployment-readiness.outputs.ready }}';
            
            let conclusion = 'neutral';
            if (status === 'passed' && securityStatus === 'passed') {
              conclusion = 'success';
            } else if (status === 'warning') {
              conclusion = 'neutral';
            } else {
              conclusion = 'failure';
            }
            
            await github.rest.checks.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              name: 'Enhanced Quality Gates',
              head_sha: context.sha,
              status: 'completed',
              conclusion: conclusion,
              output: {
                title: 'Enhanced Quality Gates Results',
                summary: `
                  ## üéØ Validation Summary
                  
                  | Gate | Status |
                  |------|--------|
                  | Quality Gates | ${status === 'passed' ? '‚úÖ' : status === 'warning' ? '‚ö†Ô∏è' : '‚ùå'} ${status.toUpperCase()} |
                  | Security Scan | ${securityStatus === 'passed' ? '‚úÖ' : '‚ùå'} ${securityStatus?.toUpperCase() || 'UNKNOWN'} |
                  | Deployment Ready | ${deploymentReady === 'true' ? '‚úÖ' : deploymentReady === 'conditional' ? '‚ö†Ô∏è' : '‚ùå'} ${deploymentReady?.toUpperCase() || 'FALSE'} |
                  
                  **Contributing Guidelines:** ‚úÖ Validated
                  **Dependabot:** ‚úÖ Configured
                  **CI/CD Enhanced:** ‚úÖ Active
                `,
                text: 'Enhanced validation pipeline with comprehensive quality gates, security scanning, and deployment readiness assessment.'
              }
            });